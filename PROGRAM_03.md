# _PROGRAM_03: NEURAL_ARCHITECTURE_

Kompletny, 12-modu≈Çowy bootcamp ≈ÇƒÖczƒÖcy fundamenty PyTorch, zaawansowane architektury neuronowe oraz budowƒô custom foundation models. Od tensor operations do enterprise-grade AI systems.

---

## üöÄ FAZA 1: FUNDAMENTY (PyTorch & Optimization)

### **Modu≈Ç 1: PyTorch Foundations & Tensor Engineering**

Ikona do wstawienia dla potwierdzenia statusu: ‚úÖ

| Lekcja | Temat G≈Ç√≥wny                                                           | Tech Stack         | Status |
| ------ | ---------------------------------------------------------------------- | ------------------ | ------ |
| **01** | Architektura PyTorch: Dlaczego Tensory rzƒÖdzƒÖ ≈õwiatem AI?              | `pytorch, tensors` |        |
| **02** | Hardware Acceleration: ZarzƒÖdzanie pamiƒôciƒÖ GPU                        | `cuda, mps, rocm`  |        |
| **03** | Operacje na tensorach: Broadcasting, Reshaping, Einstein Summation     | `einsum`           |        |
| **04** | Pydantic for Architectures: Definiowanie hiperparametr√≥w jako schematy | `pydantic`         |        |
| **05** | Projektowanie powtarzalnych eksperyment√≥w (Seeding & Determinism)      | `reproducibility`  |        |
| **06** | PROJEKT #1: Autorski silnik do przetwarzania wielowymiarowych danych   | `pytorch`          |        |

**Oczekiwane Wyniki:**

- Opanujesz fundamenty PyTorch i niskopoziomowe operacje na tensorach
- Zrozumiesz jak efektywnie zarzƒÖdzaƒá pamiƒôciƒÖ GPU w procesie treningu
- Zbudujesz powtarzalne ≈õrodowisko do eksperyment√≥w Deep Learning

---

### **Modu≈Ç 2: Autograd & Optimization Theory**

| Lekcja | Temat G≈Ç√≥wny                                                     | Tech Stack              | Status |
| ------ | ---------------------------------------------------------------- | ----------------------- | ------ |
| **01** | Mechanizm Autograd: Jak PyTorch liczy gradienty pod maskƒÖ        | `autograd`              |        |
| **02** | Funkcje straty (Loss Functions): Od MSE po Contrastive Loss      | `loss-functions`        |        |
| **03** | Optymalizatory: Anatomia SGD, Adam, AdamW i najnowszych          | `sgd, adam, lion, adan` |        |
| **04** | Schedulery: ZarzƒÖdzanie Learning Rate (Cosine Annealing, Warmup) | `lr-schedulers`         |        |
| **05** | Debugowanie gradient√≥w: Vanishing & Exploding Gradients          | `gradient-clipping`     |        |
| **06** | PROJEKT #2: Budowa w≈Çasnego optymalizatora od zera               | `custom-optimizer`      |        |

**Oczekiwane Wyniki:**

- Zrozumiesz matematykƒô stojƒÖcƒÖ za wstecznƒÖ propagacjƒÖ b≈Çƒôdu
- Nauczysz siƒô dobieraƒá i modyfikowaƒá funkcje celu do specyficznych problem√≥w AI
- Opanujesz techniki stabilizacji treningu g≈Çƒôbokich sieci neuronowych

---

## üß† FAZA 2: NEURAL ARCHITECTURES (Custom Layers & CNNs)

### **Modu≈Ç 3: Custom Layers & OOP in PyTorch**

| Lekcja | Temat G≈Ç√≥wny                                                        | Tech Stack              | Status |
| ------ | ------------------------------------------------------------------- | ----------------------- | ------ |
| **01** | nn.Module: Budowa modu≈Çowych architektur sieci neuronowych          | `nn.Module`             |        |
| **02** | Custom Layers: Tworzenie w≈Çasnych warstw (Linear, Conv, Activation) | `custom-layers`         |        |
| **03** | Inicjalizacja wag: Xavier, He i wp≈Çyw na zbie≈ºno≈õƒá modelu           | `weight-initialization` |        |
| **04** | Warstwy normalizacji: Batch Norm, Layer Norm, RMSNorm               | `normalization`         |        |
| **05** | Functional API vs Modular API: Kiedy wybieraƒá kt√≥re?                | `api-patterns`          |        |
| **06** | PROJEKT #3: Budowa hybrydowej sieci do analizy sygna≈Ç√≥w gie≈Çdowych  | `pytorch, finance`      |        |

**Oczekiwane Wyniki:**

- Zaprojektujesz autorskie warstwy sieci neuronowych w podej≈õciu obiektowym
- Wdro≈ºysz standardy normalizacji stosowane w najnowocze≈õniejszych modelach LLM
- Zrozumiesz subtelne r√≥≈ºnice w technikach inicjalizacji wag modelu

---

### **Modu≈Ç 4: Computer Vision: CNNs & Beyond**

| Lekcja | Temat G≈Ç√≥wny                                                     | Tech Stack                    | Status |
| ------ | ---------------------------------------------------------------- | ----------------------------- | ------ |
| **01** | Convolutional Neural Networks (CNN): Od LeNet po ResNet          | `cnn, resnet`                 |        |
| **02** | Downsampling & Pooling: Jak sieƒá "widzi" cechy obrazu            | `pooling, downsampling`       |        |
| **03** | Transfer Learning: Wykorzystanie wag pre-trenowanych modeli      | `transfer-learning, imagenet` |        |
| **04** | Vision Transformers (ViT): Czy uwaga (Attention) zastƒÖpi sploty? | `vit, attention`              |        |
| **05** | Augmentacja danych: Techniki zwiƒôkszania generalizacji modeli    | `data-augmentation`           |        |
| **06** | PROJEKT #4: System rozpoznawania wzorc√≥w na wykresach ≈õwiecowych | `pytorch, computer-vision`    |        |

**Oczekiwane Wyniki:**

- Zbudujesz systemy widzenia komputerowego o najwy≈ºszej precyzji
- Nauczysz siƒô adaptowaƒá potƒô≈ºne modele wizyjne do specyficznych nisz rynkowych
- Opanujesz nowoczesnƒÖ architekturƒô Vision Transformers

---

## üîÑ FAZA 3: SEQUENCE MODELS & TRANSFORMERS

### **Modu≈Ç 5: Sequence Models & Attention Mechanism**

| Lekcja | Temat G≈Ç√≥wny                                                           | Tech Stack              | Status |
| ------ | ---------------------------------------------------------------------- | ----------------------- | ------ |
| **01** | Recurrent Neural Networks (RNN/LSTM/GRU): Dlaczego od nich odchodzimy? | `rnn, lstm, gru`        |        |
| **02** | Mechanizm Uwagi (Attention): Skalowany iloczyn skalarny                | `attention-mechanism`   |        |
| **03** | Multi-Head Attention: Serce nowoczesnego AI                            | `multi-head-attention`  |        |
| **04** | Positional Encoding: Jak nauczyƒá model kolejno≈õci danych               | `positional-encoding`   |        |
| **05** | Encoder-Decoder Architecture: Fundament system√≥w t≈Çumaczenia           | `encoder-decoder`       |        |
| **06** | PROJEKT #5: Silnik t≈ÇumaczƒÖcy "Financial Talk" na "Plain English"      | `seq2seq, transformers` |        |

**Oczekiwane Wyniki:**

- Zrozumiesz matematyczne podstawy mechanizmu uwagi (Attention)
- Zaprojektujesz systemy przetwarzajƒÖce sekwencje danych o dowolnej d≈Çugo≈õci
- Opanujesz architekturƒô Encoder-Decoder niezbƒôdnƒÖ w zadaniach tekstu-na-tekst

---

### **Modu≈Ç 6: Custom Transformer Design (The SLM Blueprint)**

| Lekcja | Temat G≈Ç√≥wny                                                      | Tech Stack                  | Status |
| ------ | ----------------------------------------------------------------- | --------------------------- | ------ |
| **01** | Budowa w≈Çasnego "Small Language Model" (SLM) od zera w PyTorch    | `transformers, slm`         |        |
| **02** | Tokenizacja WordPiece vs BPE: Budowa w≈Çasnego tokenizera          | `tokenizers, bpe`           |        |
| **03** | Causal Masking: Jak sieƒá uczy siƒô przewidywaƒá nastƒôpny token      | `causal-masking`            |        |
| **04** | Optymalizacja pamiƒôci: Flash Attention i KV Caching               | `flash-attention, kv-cache` |        |
| **05** | Pre-training: Budowa pipeline'u do treningu na w≈Çasnych korpusach | `pretraining`               |        |
| **06** | PROJEKT #6: "Micro-Gemini" ‚Äì Tw√≥j w≈Çasny, lokalny model jƒôzykowy  | `pytorch, llm`              |        |

**Oczekiwane Wyniki:**

- Zbudujesz i wytrenujesz w≈Çasny model jƒôzykowy od pierwszej linii kodu
- Opanujesz krytyczne optymalizacje pamiƒôciowe (Flash Attention, KV Caching)
- Nauczysz siƒô budowaƒá w≈Çasne tokenizery dopasowane do specyficznych danych

---

## üé® FAZA 4: GENERATIVE AI & ALIGNMENT

### **Modu≈Ç 7: Generative AI: Diffusion & VAEs**

| Lekcja | Temat G≈Ç√≥wny                                                           | Tech Stack               | Status |
| ------ | ---------------------------------------------------------------------- | ------------------------ | ------ |
| **01** | Modele Dyfuzyjne: Jak szum zamienia siƒô w dane (Stable Diffusion math) | `diffusion-models`       |        |
| **02** | Variational Autoencoders (VAE): Reprezentacje w przestrzeni ukrytej    | `vae`                    |        |
| **03** | Generative Adversarial Networks (GAN): Generator vs Dyskryminator      | `gan`                    |        |
| **04** | Sampling: Techniki generowania obraz√≥w i d≈∫wiƒôku                       | `sampling-techniques`    |        |
| **05** | Image-to-Image i ControlNet: Sterowanie procesem generatywnym          | `controlnet, img2img`    |        |
| **06** | PROJEKT #7: Generator syntetycznych danych do testowania strategii     | `pytorch, generative-ai` |        |

**Oczekiwane Wyniki:**

- Zrozumiesz procesy stochastyczne stojƒÖce za generowaniem obraz√≥w i danych
- Zbudujesz generatywne modele zdolne do uzupe≈Çniania brakujƒÖcych informacji
- Stworzysz potƒô≈ºne narzƒôdzie do generowania syntetycznych danych treningowych

---

### **Modu≈Ç 8: Alignment & Fine-Tuning (RLHF)**

| Lekcja | Temat G≈Ç√≥wny                                                          | Tech Stack                 | Status |
| ------ | --------------------------------------------------------------------- | -------------------------- | ------ |
| **01** | Instrukcyjne dostrajanie (Instruction Tuning): Sterowalno≈õƒá modelu    | `instruction-tuning`       |        |
| **02** | RLHF (Reinforcement Learning from Human Feedback): PPO i DPO          | `rlhf, ppo, dpo`           |        |
| **03** | Reward Models: Budowa sieci oceniajƒÖcej jako≈õƒá odpowiedzi             | `reward-modeling`          |        |
| **04** | Parameter-Efficient Fine-Tuning (PEFT): LoRA, QLoRA w czystym PyTorch | `lora, qlora, peft`        |        |
| **05** | Ewaluacja modeli: Benchmarking (MMLU, HumanEval) i w≈Çasne metryki     | `evaluation, benchmarking` |        |
| **06** | PROJEKT #8: Dostrajanie modelu SLM do roli "Finansowego Doradcy"      | `pytorch, fine-tuning`     |        |

**Oczekiwane Wyniki:**

- Nauczysz siƒô 'wychowywaƒá' modele jƒôzykowe, by odpowiada≈Çy zgodnie z intencjƒÖ
- Wdro≈ºysz zaawansowane techniki RLHF stosowane przez gigant√≥w AI
- Opanujesz PEFT, by m√≥c trenowaƒá modele na pojedynczych kartach graficznych

---

## üï∏Ô∏è FAZA 5: ADVANCED ARCHITECTURES (GNN & Distributed)

### **Modu≈Ç 9: Graph Neural Networks (GNN)**

| Lekcja | Temat G≈Ç√≥wny                                                  | Tech Stack               | Status |
| ------ | ------------------------------------------------------------- | ------------------------ | ------ |
| **01** | Reprezentacja danych jako grafy: Wƒôz≈Çy, krawƒôdzie, sƒÖsiedztwo | `graph-theory`           |        |
| **02** | Message Passing: Jak informacje p≈ÇynƒÖ przez sieƒá grafowƒÖ      | `message-passing`        |        |
| **03** | Graph Convolutional Networks (GCN) i Graph Attention (GAT)    | `gcn, gat`               |        |
| **04** | Zastosowanie: Wykrywanie prania pieniƒôdzy i analiza ryzyka    | `fraud-detection`        |        |
| **05** | Praca z bibliotekƒÖ PyTorch Geometric                          | `torch-geometric`        |        |
| **06** | PROJEKT #9: System detekcji oszustw w sieciach transakcyjnych | `pytorch-geometric, gnn` |        |

**Oczekiwane Wyniki:**

- Zbudujesz modele zdolne do analizy skomplikowanych powiƒÖza≈Ñ sieciowych
- Opanujesz standardy PyTorch Geometric do pracy z danymi grafowymi
- Wdro≈ºysz systemy AI wykrywajƒÖce anomalie w strukturach relacyjnych

---

### **Modu≈Ç 10: Scaling & Distributed Training**

| Lekcja | Temat G≈Ç√≥wny                                                     | Tech Stack                  | Status |
| ------ | ---------------------------------------------------------------- | --------------------------- | ------ |
| **01** | Data Parallelism (DP) vs Distributed Data Parallel (DDP)         | `ddp, data-parallel`        |        |
| **02** | Model Parallelism & Pipeline Parallelism: Trenowanie gigant√≥w    | `model-parallel, pipeline`  |        |
| **03** | Techniki Mixed Precision (FP16/BF16): Przyspieszanie treningu    | `mixed-precision, amp`      |        |
| **04** | Checkpointing: Zapisywanie i wznawianie gigantycznych proces√≥w   | `checkpointing`             |        |
| **05** | Profilowanie PyTorcha: Znajdowanie wƒÖskich garde≈Ç                | `profiling, bottleneck`     |        |
| **06** | PROJEKT #10: Uruchomienie rozproszonego treningu na klastrze GPU | `ddp, distributed-training` |        |

**Oczekiwane Wyniki:**

- Bƒôdziesz wiedzieƒá, jak skalowaƒá trening modeli na wiele kart graficznych i maszyn
- Drastycznie przyspieszysz procesy treningowe dziƒôki automatycznej mieszanej precyzji
- Nauczysz siƒô profilowaƒá kod AI, by eliminowaƒá przestoje w wykorzystaniu GPU

---

## üîß FAZA 6: OPTIMIZATION & DEPLOYMENT

### **Modu≈Ç 11: Model Optimization & Quantization**

| Lekcja | Temat G≈Ç√≥wny                                                               | Tech Stack                      | Status |
| ------ | -------------------------------------------------------------------------- | ------------------------------- | ------ |
| **01** | Post-Training Quantization (PTQ): Przej≈õcie z FP32 na INT8/INT4            | `quantization, ptq`             |        |
| **02** | Quantization Aware Training (QAT): Trening uwzglƒôdniajƒÖcy b≈Çƒôdy            | `qat`                           |        |
| **03** | Pruning: Wycinanie niepotrzebnych wag (Sparsity)                           | `pruning, sparsity`             |        |
| **04** | Knowledge Distillation: Przelewanie wiedzy z giganta (Teacher) do studenta | `knowledge-distillation`        |        |
| **05** | Eksport modeli: ONNX, TensorRT i kompilacja PyTorch 2.0                    | `onnx, tensorrt, torch-compile` |        |
| **06** | PROJEKT #11: Optymalizacja modelu pod kƒÖtem urzƒÖdze≈Ñ Edge                  | `edge-deployment`               |        |

**Oczekiwane Wyniki:**

- Odchudzisz modele AI bez znaczƒÖcej utraty jako≈õci odpowiedzi
- Przygotujesz swoje modele do pracy na procesorach mobilnych i edge
- Opanujesz techniki destylacji wiedzy stosowane w Small Language Models

---

## üèÜ FAZA 7: CAPSTONE PROJECT

### **Modu≈Ç 12: CAPSTONE: Custom Foundation Model**

| Lekcja | Temat G≈Ç√≥wny                                                           | Tech Stack                | Status |
| ------ | ---------------------------------------------------------------------- | ------------------------- | ------ |
| **01** | System Design: Projektowanie autorskiej architektury sieci neuronowej  | `system-architecture`     |        |
| **02** | Przygotowanie Datasetu: Skalowanie, czyszczenie i tokenizacja danych   | `data-preparation`        |        |
| **03** | Trening: Przeprowadzenie pe≈Çnego procesu z monitoringiem i logowaniem  | `training-pipeline`       |        |
| **04** | Ewaluacja i Fine-tuning: Optymalizacja pod konkretne zadanie biznesowe | `evaluation, fine-tuning` |        |
| **05** | Deployment: Pakowanie modelu w Docker i serwowanie przez TorchServe    | `docker, torchserve`      |        |
| **06** | WIELKI FINA≈Å: Prezentacja autorskiej sieci neuronowej klasy Enterprise | `presentation`            |        |

**Oczekiwane Wyniki:**

- Stworzysz od zera unikalnƒÖ architekturƒô sieci neuronowej dla biznesu
- Zbudujesz potƒô≈ºne portfolio architekta Deep Learning
- Zyskasz kompetencje pozwalajƒÖce na projektowanie autorskich modeli fundamentalnych

---

## üìä Podsumowanie Bootcampu

- **Ca≈Çkowity czas nauki:** 2x / tydz. (90‚Äì120 min) x 5 mc = 63-84 godzin
- **Liczba modu≈Ç√≥w:** 12
- **Liczba lekcji:** 72

### **Stack Technologiczny:**

**Core PyTorch:**

- `pytorch, tensors, autograd`
- `nn.Module, custom-layers`
- `cuda, mps, rocm`
- `pydantic`

**Optimization:**

- `sgd, adam, adamw, lion, adan`
- `lr-schedulers`
- `gradient-clipping`
- `mixed-precision (fp16/bf16)`

**Computer Vision:**

- `cnn, resnet`
- `vision-transformers (vit)`
- `transfer-learning`
- `data-augmentation`

**Sequence Models & Transformers:**

- `rnn, lstm, gru`
- `attention-mechanism`
- `multi-head-attention`
- `encoder-decoder`
- `tokenizers (bpe, wordpiece)`
- `flash-attention, kv-cache`

**Generative AI:**

- `diffusion-models`
- `vae, gan`
- `controlnet`
- `sampling-techniques`

**Alignment & Fine-Tuning:**

- `instruction-tuning`
- `rlhf, ppo, dpo`
- `lora, qlora, peft`
- `reward-modeling`

**Advanced Architectures:**

- `graph-neural-networks (gcn, gat)`
- `torch-geometric`
- `message-passing`

**Distributed Training:**

- `ddp (distributed-data-parallel)`
- `model-parallel, pipeline-parallel`
- `checkpointing`
- `profiling`

**Optimization & Deployment:**

- `quantization (ptq, qat)`
- `pruning, sparsity`
- `knowledge-distillation`
- `onnx, tensorrt`
- `torch-compile`
- `torchserve, docker`

### **Po uko≈Ñczeniu bƒôdziesz potrafiƒá:**

‚úÖ Operowaƒá na tensorach z pe≈ÇnƒÖ kontrolƒÖ nad pamiƒôciƒÖ GPU  
‚úÖ Budowaƒá autorskie optymalizatory i funkcje straty  
‚úÖ Projektowaƒá custom layers i modu≈Çy neuronowe od zera  
‚úÖ Implementowaƒá state-of-the-art architektury CNN i Vision Transformers  
‚úÖ Rozumieƒá matematykƒô mechanizmu uwagi (Attention)  
‚úÖ Budowaƒá w≈Çasne Small Language Models (SLM) od podstaw  
‚úÖ Tworzyƒá generatywne modele (Diffusion, VAE, GAN)  
‚úÖ Stosowaƒá zaawansowane techniki fine-tuningu (RLHF, LoRA, PEFT)  
‚úÖ Implementowaƒá Graph Neural Networks dla z≈Ço≈ºonych struktur relacyjnych  
‚úÖ Skalowaƒá trening na wiele GPU i maszyn (DDP, Model Parallelism)  
‚úÖ Optymalizowaƒá modele przez quantization, pruning i distillation  
‚úÖ Deployowaƒá modele production-grade przez TorchServe i ONNX  
‚úÖ Projektowaƒá autorskie foundation models klasy enterprise

---

**Author:** TakzenAI: Krzysztof Pika
